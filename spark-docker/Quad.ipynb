{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=QuadrantCount, master=local[*]) created by __init__ at <ipython-input-1-6ce16d6691db>:39 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-02f4dee3b9f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Create a local StreamingContext with two working thread and batch interval of 1 second\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[2]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NetworkWordCount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# ssc = StreamingContext(sc, 30)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    306\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 308\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    309\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=QuadrantCount, master=local[*]) created by __init__ at <ipython-input-1-6ce16d6691db>:39 "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "# sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "# ssc = StreamingContext(sc, 30)\n",
    "\n",
    "socket_stream = ssc.socketTextStream(\"127.0.0.1\", 9999)\n",
    "\n",
    "# Function to map the point to the right quadrant\n",
    "def get_quadrant(line):\n",
    "    # Convert the input string into a pair of numbers\n",
    "    try:\n",
    "        (x, y) = [float(x) for x in line.split()]\n",
    "    except:\n",
    "        print (\"Invalid input\")\n",
    "        return ('Invalid points', 1)\n",
    "\n",
    "    # Map the pair of numbers to the right quadrant\n",
    "    if x > 0 and y > 0:\n",
    "        quadrant = 'First quadrant'\n",
    "    elif x < 0 and y > 0:\n",
    "        quadrant = 'Second quadrant'\n",
    "    elif x < 0 and y < 0:\n",
    "        quadrant = 'Third quadrant'\n",
    "    elif x > 0 and y < 0:\n",
    "        quadrant = 'Fourth quadrant'\n",
    "    elif x == 0 and y != 0:\n",
    "        quadrant = 'Lies on Y axis'\n",
    "    elif x != 0 and y == 0:\n",
    "        quadrant = 'Lies on X axis'\n",
    "    else:\n",
    "        quadrant = 'Origin'\n",
    "\n",
    "    # The pair represents the quadrant and the counter increment\n",
    "    return (quadrant, 1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    if len(sys.argv) != 3:\n",
    "        raise IOError(\"Invalid usage; the correct format is:\\nquadrant_count.py <hostname> <port>\")\n",
    "\n",
    "    # Initialize a SparkContext with a name\n",
    "    spc = SparkContext(appName=\"QuadrantCount\")\n",
    "\n",
    "    # Create a StreamingContext with a batch interval of 2 seconds\n",
    "    stc = StreamingContext(spc, 2)\n",
    "\n",
    "    # Checkpointing feature\n",
    "    stc.checkpoint(\"checkpoint\")\n",
    "\n",
    "    # Creating a DStream to connect to hostname:port (like localhost:9999)\n",
    "    lines = stc.socketTextStream(sys.argv[1], int(sys.argv[2]))\n",
    "\n",
    "    # Function that's used to update the state\n",
    "    updateFunction = lambda new_values, running_count: sum(new_values) + (running_count or 0)\n",
    "\n",
    "    # Update all the current counts of number of points in each quadrant\n",
    "    running_counts = lines.map(get_quadrant).updateStateByKey(updateFunction)\n",
    "\n",
    "    # Print the current state\n",
    "    running_counts.pprint()\n",
    "    \n",
    "    # Start the computation\n",
    "    stc.start()\n",
    "\n",
    "    # Wait for the computation to terminate\n",
    "    stc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
